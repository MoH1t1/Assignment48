{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n\nLasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of regularized linear regression that uses L1 regularization. \nIt adds a penalty proportional to the absolute value of the coefficients to the cost function, which can shrink some coefficients to zero, effectively\nperforming feature selection. Unlike ordinary linear regression, Lasso reduces overfitting by keeping only the most relevant features.\n\n# Q2. What is the main advantage of using Lasso Regression in feature selection?\n    \nThe main advantage is that Lasso can eliminate irrelevant features by shrinking their coefficients to zero, automatically performing feature selection and \nsimplifying the model.\n\n# Q3. How do you interpret the coefficients of a Lasso Regression model?\n    \nThe coefficients in Lasso Regression represent the impact of each feature on the target variable. Larger coefficients indicate stronger influence.\nFeatures with coefficients shrunk to zero are effectively excluded from the model.\n\n# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n\nThe main tuning parameter is 位 (regularization strength). A larger 位 increases the penalty, shrinking more coefficients toward zero and making the model \nsimpler (potentially underfitting). A smaller 位 reduces the penalty, allowing the model to fit the training data more closely (potentially overfitting).\n\n# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n\nLasso Regression is inherently linear, but it can be used for non-linear problems by transforming the input features (e.g., using polynomial features or\nkernel methods) to create non-linear relationships.\n\n# Q6. What is the difference between Ridge Regression and Lasso Regression?\n    \nRidge Regression uses L2 regularization, shrinking coefficients but not setting them to zero.\nLasso Regression uses L1 regularization, which can shrink some coefficients to zero, performing feature selection.\n\n# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n\nYes, Lasso Regression can handle multicollinearity by shrinking the coefficients of correlated features, potentially eliminating some of them entirely\n(if their coefficients become zero), thus reducing multicollinearity's impact.\n\n# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n\nThe optimal value of 位 is typically chosen using cross-validation. A range of values is tested, and the one that minimizes the model's generalization error \n(on validation data) is selected.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}